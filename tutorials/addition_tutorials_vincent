详细介绍
文本分类需要CNN？ No！fastText完美解决你的需求
http://blog.csdn.net/weixin_36604953/article/details/78195462
http://blog.csdn.net/weixin_36604953/article/details/78324834

语言模型就是检测语言的逻辑性，词语前后关系的重要性，考察一个句子出现的可能性（也就是概率）。

如果一个句子S由n个词w1~wn，那么S出现的概率就应该等于P(w1,w2,…,wn)。

词语wn出现的概率依赖于它前面n−1个词。当n很大时，P(wn│w1,w2,…,w(n−1))的计算是非常麻烦，
马尔科夫假设的概念并由此得到“二元模型”。马尔科夫假设的意思是“任意一个词w_k只与它前面的词即w(k−1)有关”。

语言模型正是为了考察句子，或者说由词a到词b存在在句子中的概率而存在的。如果将我们的语言模型即为f，
那么word2vec就是去训练这个f，不关注这个模型是否完美，而是要获取模型产生的参数，
对于word2vec，就是神经网络的权重参数，将这些参数作为句子S的替代，由这些参数组成的向量，就是我们所谓的“词向量”。这也就是word2vec的输出。

Skip-gram是通过一个词a去预测它周围的上下文； 数据量较大的情况
而CBOW相反，是通过上下文来预测其间的词。   一般数据

Bow模型认为“我爱你”和“你爱我”是相同的
BoW 使用一组无序的单词（word）来表达一段文字或一个文档，并且文档中每个单词的出现都是独立的。


fastText的速度快，且不会受到不平衡分类问题影响的关键因素就是“霍夫曼树”的数据结构


霍夫曼编码树，又称最优二叉树。是一类带权路径长度最短的树。
假设有n个权值{w1,w2,…,wn}，如果构造一棵有n个叶子节点的二叉树，而这n个叶子节点的权值是{w1,w2,…,wn}，则所构造出的带权路径长度最小的二叉树就被称为霍夫曼树。

带权路径长度：如果在一棵二叉树中共有n个叶子节点，用Wi表示第i个叶子节点的权值，Li表示第i个叶子节点到根节点的路径长度，

则该二叉树的带权路径长度：WPL=W1∗L1+W2∗L2+...Wn∗Ln

fastText的霍夫曼树叶子结点对应为最终的label，可以看到，权重最大的，也就是权重最大的label，其深度最小，fastText 充分利用了这个性质，使得其速度得以提升。 

Huffman编码（霍夫曼编码）： 
对每个字符设计长度不等的编码，让出现较多的字符采用尽可能短的编码。利用霍夫曼树求得的用于通信的二进制编码称为霍夫曼编码。 


fastText算法是一种有监督的模型，与CBOW架构很相似。
CBOW，通过上下文预测中间词，而fastText则是通过上下文预测标签（这个标签就是文本的类别，是训练模型之前通过人工标注等方法事先确定下来的）。


fastText模型的输入是一个词的序列（一段文本或者一句话)，输出是这个词序列属于不同类别的概率。
在序列中的词和词组构成特征向量，特征向量通过线性变换映射到中间层，再由中间层映射到标签。
fastText在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。


当数据量巨大时，线性分类器的计算十分昂贵，所以fastText使用了一个基于霍夫曼编码树的分层softmax方法。


常用的文本特征表示方法是词袋模型，然而词袋（BoW）中的词顺序是不变的，但是明确考虑该顺序的计算成本通常十分高昂。
作为替代，fastText使用n-gram获取额外特征来得到关于局部词顺序的部分信息，后文将详细介绍。 







